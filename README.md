# Ponderadas
Contains proposed solutions for the evaluated exercises

# 1. Ponderada 1

Passos para a criaÃ§Ã£o da imagem:

Criando um ambiente virtual:

```bash
python3 -m venv venv
source /venv/bin/activate
mkdir src && cd src
```

Instalando as dependÃªncias:

```bash
pip install fastapi
pip install "uvicorn[standard]"
pip freeze > requirements.txt
```

ApÃ³s a criaÃ§Ã£o do Dockerfile, executar os seguintes comandos no terminal:

```bash
docker build -t python-img .
docker run -p 3000:80 --name fastapi-server python-img

```

O container jÃ¡ estÃ¡ rodando nossa aplicaÃ§Ã£o em FASTAPI ðŸ¥³!

Publicando a imagem no Dockerhub:

```bash
docker login
docker tag python-img hallzero/inteli:0.0.x
docker push hallzero/inteli:0.0.x
```

# 2. Ponderada 2

Este projeto propÃµe uma soluÃ§Ã£o para o exercÃ­cio ponderado 2 utilizando express como API, sequelize como ORM, postgres para o Banco de Dados, e HTML puro (AtÃ© o momento) para o frontend.

# Arquivo docker-compose.yaml - ExplicaÃ§Ã£o

Este arquivo `docker-compose.yaml` contÃ©m a definiÃ§Ã£o de um ambiente Docker composto por dois serviÃ§os: um banco de dados PostgreSQL e uma API Express.

## ServiÃ§o `db` - Banco de Dados PostgreSQL

Este serviÃ§o utiliza a imagem oficial do PostgreSQL versÃ£o 15.4. Ele fornece um banco de dados persistente para a aplicaÃ§Ã£o.

## ServiÃ§o `api` - API Express

Este serviÃ§o utiliza uma imagem chamada `hallzero/express-p2:latest`, contendo uma aplicaÃ§Ã£o Express para a API.

## Como Usar

1. Certifique-se de ter o Docker e o Docker Compose instalados em seu sistema.
2. Clone este repositÃ³rio para sua mÃ¡quina, se ainda nÃ£o o tiver feito.
3. Navegue atÃ© o diretÃ³rio onde o arquivo `docker-compose.yaml` estÃ¡ localizado.
4. Execute o comando `docker-compose up -d` para iniciar os serviÃ§os em segundo plano.
5. Os serviÃ§os serÃ£o iniciados e a API estarÃ¡ disponÃ­vel em `http://localhost:3000`.

# Arquivo database.js - ExplicaÃ§Ã£o

Este arquivo contÃ©m a configuraÃ§Ã£o e a definiÃ§Ã£o dos modelos de dados utilizando o mÃ³dulo Sequelize para interagir com um banco de dados PostgreSQL.

## ConexÃ£o com o Banco de Dados

O cÃ³digo inicia importando os mÃ³dulos `Sequelize` e `DataTypes` do pacote 'sequelize'. Em seguida, uma instÃ¢ncia do Sequelize Ã© criada para se conectar ao banco de dados PostgreSQL. As configuraÃ§Ãµes de conexÃ£o incluem o nome do banco de dados, o nome de usuÃ¡rio, a senha, o host e a porta.

A funÃ§Ã£o `sequelize.authenticate()` Ã© usada para verificar se a conexÃ£o com o banco de dados Ã© bem-sucedida. Se a autenticaÃ§Ã£o for bem-sucedida, uma mensagem "Connected to the database" serÃ¡ exibida. Caso contrÃ¡rio, um erro serÃ¡ logado.

## Modelos de Dados

### Modelo 'User'

O primeiro modelo definido Ã© 'User', que representa informaÃ§Ãµes de um usuÃ¡rio. Ele possui as seguintes colunas:

- `id`: Um nÃºmero inteiro com incremento automÃ¡tico e chave primÃ¡ria.
- `name`: Uma string que nÃ£o pode ser nula (ou seja, Ã© obrigatÃ³ria).
- `password`: Uma string para armazenar a senha do usuÃ¡rio.

### Modelo 'Task'

O segundo modelo definido Ã© 'Task', que representa uma tarefa. Ele possui as seguintes colunas:

- `id`: Um nÃºmero inteiro com incremento automÃ¡tico e chave primÃ¡ria.
- `description`: Uma string para armazenar a descriÃ§Ã£o da tarefa.
- `done`: Um valor booleano para indicar se a tarefa estÃ¡ concluÃ­da.
- `user_id`: Uma referÃªncia a um usuÃ¡rio por meio da coluna `id` do modelo 'User'.

## InicializaÃ§Ã£o e SincronizaÃ§Ã£o

A funÃ§Ã£o `init()` Ã© assÃ­ncrona e executa a sincronizaÃ§Ã£o dos modelos com o banco de dados usando `sequelize.sync({ force: true })`. Isso cria as tabelas correspondentes aos modelos. Um usuÃ¡rio de exemplo Ã© criado com o nome 'teste' e senha 'teste123' usando `User.create()`.

# Arquivo login.js - ExplicaÃ§Ã£o

Este arquivo contÃ©m um mÃ³dulo responsÃ¡vel por lidar com o processo de autenticaÃ§Ã£o de um usuÃ¡rio e a geraÃ§Ã£o de um token de acesso usando a biblioteca `jsonwebtoken`. O objetivo Ã© validar as credenciais do usuÃ¡rio e fornecer um token para acesso autenticado.

## VariÃ¡veis de Ambiente

As variÃ¡veis de ambiente `JWT_SECRET` e `JWT_ALGORITHM` sÃ£o carregadas a partir do arquivo `.env` usando `process.env`. Elas representam a chave secreta usada para assinar o token e o algoritmo de criptografia usado na geraÃ§Ã£o do token, respectivamente.

O fluxo do cÃ³digo Ã© o seguinte:

1. O cÃ³digo chama a funÃ§Ã£o `readUserByName()` para buscar informaÃ§Ãµes do usuÃ¡rio com base no nome fornecido na solicitaÃ§Ã£o.

2. Se nenhum usuÃ¡rio for encontrado ou se a senha fornecida na solicitaÃ§Ã£o nÃ£o coincidir com a senha armazenada no banco de dados, uma resposta de erro com status 403 (Acesso Proibido) Ã© enviada de volta ao cliente, indicando que o login Ã© invÃ¡lido.

3. Se o usuÃ¡rio for encontrado e as credenciais estiverem corretas, a senha do objeto `user` Ã© excluÃ­da para evitar o envio dela de volta como resposta.

4. Um token de acesso Ã© gerado usando `jwt.sign()`, que recebe o nome do usuÃ¡rio, a chave secreta e o algoritmo como argumentos. O token gerado Ã© incluÃ­do em uma resposta JSON enviada de volta ao cliente.

## Uso

Este mÃ³dulo Ã© projetado para ser chamado em uma rota que lida com a autenticaÃ§Ã£o do usuÃ¡rio. O nome de usuÃ¡rio e senha sÃ£o fornecidos na solicitaÃ§Ã£o e o mÃ³dulo executa a autenticaÃ§Ã£o, retornando um token de acesso em caso de sucesso.

# 3. Ponderada 3

Este projeto tem como objetivo a produÃ§Ã£o de um modelo preditivo e uma API que, a partir dos dados passados, consiga prever um valor.

## Estrutura de Pastas:

Todo o processo de Limpeza de dados e treinamento estÃ¡ contido em 'machine-learning.ipynb', de onde Ã© gerado o modelo propriamente dito em um arquivo .pkl 'finalized_model.pkl'.

```
â””â”€â”€ ponderada3/
    â””â”€â”€ src/
        â”œâ”€â”€ dataset/
        â”‚   â””â”€â”€ Global YouTube Statistics.csv
        â”œâ”€â”€ machine-learning/
        â”‚   â”œâ”€â”€ finalized_model.pkl
        â”‚   â””â”€â”€ machine-learning.ipynb
        â”œâ”€â”€ app.py
        â”œâ”€â”€ Dockerfile
        â””â”€â”€ requirements.txt
```

## Dataset utilizado: Global Youtube Statistics
O dataset pode ser encontrado em: https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023

## Machine Learning Algorithm: Linear Regression

A regressÃ£o linear Ã© um dos modelos de aprendizado de mÃ¡quina mais simples e interpretÃ¡veis. Ã‰ fÃ¡cil de entender e implementar, o que o torna uma excelente escolha para nÃ³s. (AtÃ© porque odeio anÃ¡lise de dados e nÃ£o quero ir alÃ©m disso... Desculpa Murilo ðŸ˜…)

No contexto de prever visualizaÃ§Ãµes de vÃ­deos no YouTube, isso significa encontrar uma equaÃ§Ã£o que descreve como os atributos dos vÃ­deos (os dados usados para a prediÃ§Ã£o) se relacionam com o nÃºmero de visualizaÃ§Ãµes. Ã‰ uma tÃ©cnica simples e interpretÃ¡vel que pode ajudar a entender e otimizar o desempenho dos vÃ­deos.

## API: FastAPI

A API possui apenas duas rotas:
- GET ('/') -> Retorna um Hello World simples
- POST ('/predict') -> A partir dos dados passados para o modelo, retorna uma prediÃ§Ã£o do video_views do canal em questÃ£o

### Para rodar o projeto:

1. Baixe a imagem do repositÃ³rio hallzero/predict [aqui!](https://hub.docker.com/repository/docker/hallzero/prediction/general)
2. Rode o comando no terminal:
```
docker run -p 8000:8000 --name nome_exemplo hallzero/predict:0.0.1
```
3. O FastAPI gera uma documentaÃ§Ã£o automÃ¡tica em ('/docs'). Na aba explicando o ('/predict'), utilize o template de input para fazer a prediÃ§Ã£o, que serÃ¡ devolvida como a respota Ã  requisiÃ§Ã£o.

Pontos de melhoria observados:
- Preciso aprender melhor a lidar com dados categÃ³ricos
- Aprender a modularizar o processo de limpeza dos dados, a fim de nÃ£o depender de rodar o notebook ou colocar o mesmo cÃ³digo na API

# Ponderada 4

```
.
â””â”€â”€ ponderada4/
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ database.js
    â”œâ”€â”€ machine-learning/
    â”‚   â”œâ”€â”€ finalized_model.pkl
    â”‚   â”œâ”€â”€ machine-learning.ipynb
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â””â”€â”€ requirements.txt
    â”œâ”€â”€ media/
    â”‚   â””â”€â”€ Screencast from 02-10-2023 23:49:03.webm
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ task.js
    â”‚   â””â”€â”€ user.js
    â”œâ”€â”€ node_modules
    â”œâ”€â”€ public/
    â”‚   â”œâ”€â”€ charts.html
    â”‚   â”œâ”€â”€ login.html
    â”‚   â””â”€â”€ todo.html
    â”œâ”€â”€ routes/
    â”‚   â”œâ”€â”€ login.js
    â”‚   â””â”€â”€ newTask.js
    â”œâ”€â”€ .env
    â”œâ”€â”€ app.js
    â”œâ”€â”€ docker-compose.yaml
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ package-lock.json
    â””â”€â”€ package.json
```

# Ponderada 5 - Resenha sobre o artigo
A partir da leitura do Artigo "Machine learning for internet of things data analysis: a survey", podemos traÃ§ar comparativos entre os conceitos nele apresentados, as atividades ponderadas e o projeto desenvolvido durante o mÃ³dulo. Nesse contexto, podemos evidenciar tÃ³picos:

## IoT, Smart Data e Dados

Segundo o artigo, o propÃ³sito da Internet of Things (IoT) Ã© desenvolver ambientes inteligentes e simplificar o estilo de vida ao poupar tempo, energia e dinheiro, conscintindo em dispositivos conectados uns aos outros para aprimorar suas performaces. Dessa forma, o processo de coletar dados atravÃ©s de sensores, extrair informaÃ§Ãµes a partir deles e, por fim transferir esse conhecimento para outros objetos, dispoitivos e servidores Ã© fundamental. Nesse contexto, um cenÃ¡rio especificado Ã© a aplicaÃ§Ã£o dessas tecnologias para a reduÃ§Ã£o de custos dentro de uma empresa, que relaciona-se diretamente com o projeto do mÃ³dulo. AlÃ©m disso, hÃ¡ a explicaÃ§Ã£o de protocolos de comunicaÃ§Ã£o (D2D, D2S, S2S), que correspondem a uma parte do transporte dos dados, e o destaque do processamento e preparaÃ§Ã£o dos dados (Analytics at the edge, stream analisys, IoT analysis at the database) para reforÃ§ar as diferentes necessidades de um projeto. No nosso caso em particular, Ã© importante destacar o Cloud Computing, que embora nÃ£o seja a maneira primÃ¡ria de computaÃ§Ã£o do treinamento dos modelos, pode ser uma alternativa.

Smart Data: Por ser de extrema importÃ¢ncia para gerar bons insights, a qualidade dos dados deve ser garantida. Devido ao grande volume de dados brutos nÃ£o necessariamente Ãºteis ou preparados para anÃ¡lise, deve-se construir uma camada de abstraÃ§Ã£o a mais (feature engeneering), bem como garantir a seguranÃ§a dos dados (JWT, por exemplo) para tornÃ¡-los mais inteligentes.

## Smart city
O artigo exemplifica diversos conceitos e vantagens do uso do IoT a partir de cidades inteligentes. Embora nÃ£o seja diretamente relacionado, podemos traÃ§ar alguns paralelos com o projeto desenvolvido:

- Smart Energy: Reduzir o consume de energia geral atravÃ©s da mediÃ§Ã£o de sensores. (ConsequÃªncia direta)
- Smart Mobility: Efeitos diversos na forma de rodar veÃ­culos, principalmente ao monitorar sua performance. (Objetivo principal)
- Urban Planning: Ajudar em decisÃµes de longo-prazo. Ao coletar dados de diferentes fontes, Ã© possÃ­vel fazer decisÃµes para o futuro e  prediÃ§Ãµes para problemas potenciais. (ConsequÃªncia direta)
- Smart city data characteristhics: Gera dados de maneira contÃ­nua e volumosa. Processar os dados com diferentes fontes, frequÃªncias e intregÃ¡-los conscistentemente e com qualidade pode ser desafiador. (CaracterÃ­stica dos dados disponibilizados)
- Quality of Information (QoI): Necessidade de extrair nÃ­veis mais altos de abstraÃ§Ã£o e prover informaÃ§Ãµes acionÃ¡vies para outros serviÃ§os. Selecionar fontes confiÃ¡veis e combina-los Ã© de suma importÃ¢ncia. (Processo)

## Taxonomia dos algoritmos de machine learning
Devido Ã  geraÃ§Ã£o massiva de dados por diferentes fontes, Ã© importante garantir a manuseabilidade de suas caracterÃ­sticas a fim de garantir que algoritmos de machine learning sejam aplicados no escopo de smart data em projetos IoT. Em ambos projeto e atividades ponderadas, modelos de machine learning estavam presentes e a utilizaÃ§Ã£o pontual dependia diretamente da finalidade e caracterÃ­stica dos dados. As abordagens foram majoritariamente para aprendizado supervisionado e explorou-se diversas opÃ§Ãµes de modelos e ferramentas.

Nesse contexto, trabalhamos com abordagens de sÃ©ries temporais, aplicando conceitos. NÃ£o necessariamente todos os testes deram certo, mas constituiram experiÃªncias interessantes. A seguir, destaco alguns dos conceitos mais importantes:

- RegressÃ£o Linear: Utilizado na prediÃ§Ã£o de visualizaÃ§Ãµes de um canal nas atividades ponderadas. 
- Exponential Smoothing: MÃ©todo que utiliza sÃ©ries temporais, aplicando pesos maiores Ã  dados mais recentes. Ideal para forecast de valores imediatos.
- Sliding-window method: Citado no artigo na sessÃ£o de sÃ©ries temporais, Ã© um dos principais componentes das anÃ¡lises do grupo. Consciste na aplicaÃ§Ã£o de uma janela de tempo para determinar o comportamento de uma falha.

- Pycaret: Ferramenta que possibilitou a aplicaÃ§Ã£o de todo o ciclo da criaÃ§Ã£o de um modelo rapidamente.
- Prophet: Framework open-source do Facebook, Ã© uma das ferramentas utilizadas para de realizar previsÃµes em sÃ©ries temporais. 

## ConclusÃµes
O artigo conclui que para extrair conhecimento dos dados coletados, vÃ¡rios algorÃ­tmos podem ser aplicados. Escolher um algoritmo adequado para aplicaÃ§Ãµes IoT especÃ­ficas Ã© um tÃ³pico importante, sendo que diferentes aplicaÃ§Ãµes possuem diferentes caracterÃ­sticas, com dados com propriedades distintas a serem consideradas e a taxonomia Ã© outro faotr importante na aplicaÃ§Ã£o de anÃ¡lise de dados pata smart data.

Da mesma forma, Ã© possÃ­vel obter essas noÃ§Ãµes no escopo prÃ¡tico do projeto, uma vez que o contexto Ã© objetivamente parecido e os processos que levam Ã  construÃ§Ã£o da aplicaÃ§Ã£o final sÃ£o constituÃ­dos pela utilizaÃ§Ã£o dos pontos evidenciados. Inclusivamente, as atividades ponderadas, embora num contexto mais simples, tambÃ©m apresentam um nÃ­vel de complexidade que exige os conhecimentos apresentados no artigo.
